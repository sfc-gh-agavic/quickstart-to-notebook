{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Getting Started with Snowflake Dynamic Tables\n",
        "\n",
        "This notebook is based on the [Snowflake Dynamic Tables Quickstart Guide](https://quickstarts.snowflake.com/guide/getting_started_with_dynamic_tables/index.html).\n",
        "\n",
        "## Overview\n",
        "\n",
        "Dynamic Tables in Snowflake are a powerful feature that enables you to create automated, continuously updated materialized views. This tutorial will walk you through:\n",
        "\n",
        "- Setting up your Snowflake environment\n",
        "- Creating sample data using Python User-Defined Table Functions (UDTFs)\n",
        "- Building dynamic tables for data transformations\n",
        "- Implementing data validation and monitoring\n",
        "- Managing and optimizing dynamic table pipelines\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Before starting this tutorial, ensure you have:\n",
        "- Access to a Snowflake account ([30-day free trial available](https://trial.snowflake.com))\n",
        "- Basic knowledge of SQL and database concepts\n",
        "- Basic understanding of Python programming\n",
        "- Snowflake Python connector installed: `pip install snowflake-connector-python`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Snowflake Connection Setup\n",
        "\n",
        "First, let's establish a connection to Snowflake. Make sure to update the connection parameters with your Snowflake account details.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import snowflake.connector\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "# Snowflake connection parameters - UPDATE THESE WITH YOUR DETAILS\n",
        "connection_params = {\n",
        "    'account': 'your-account-identifier',  # e.g., 'abc12345.us-east-1'\n",
        "    'user': 'your-username',\n",
        "    'password': 'your-password',\n",
        "    'warehouse': 'COMPUTE_WH',  # We'll create our own later\n",
        "    'database': 'SNOWFLAKE_SAMPLE_DATA',\n",
        "    'schema': 'PUBLIC'\n",
        "}\n",
        "\n",
        "# Create connection\n",
        "conn = snowflake.connector.connect(**connection_params)\n",
        "cursor = conn.cursor()\n",
        "\n",
        "print(\"✅ Connected to Snowflake successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Environment Setup\n",
        "\n",
        "Now let's create the necessary database, schema, and warehouse for our Dynamic Tables demo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper function to execute SQL commands\n",
        "def execute_sql(sql, description=\"\"):\n",
        "    \"\"\"Execute SQL command and display results.\"\"\"\n",
        "    try:\n",
        "        cursor.execute(sql)\n",
        "        if description:\n",
        "            print(f\"✅ {description}\")\n",
        "        \n",
        "        # Fetch results if it's a SELECT statement\n",
        "        if sql.strip().upper().startswith('SELECT') or sql.strip().upper().startswith('SHOW'):\n",
        "            results = cursor.fetchall()\n",
        "            columns = [desc[0] for desc in cursor.description]\n",
        "            df = pd.DataFrame(results, columns=columns)\n",
        "            display(df)\n",
        "            return df\n",
        "        else:\n",
        "            results = cursor.fetchall()\n",
        "            if results:\n",
        "                print(f\"Result: {results}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error: {e}\")\n",
        "        \n",
        "# Create database and schema\n",
        "execute_sql(\"CREATE DATABASE IF NOT EXISTS DEMO;\", \"Created DEMO database\")\n",
        "execute_sql(\"CREATE SCHEMA IF NOT EXISTS DEMO.DT_DEMO;\", \"Created DT_DEMO schema\")\n",
        "execute_sql(\"USE SCHEMA DEMO.DT_DEMO;\", \"Using DEMO.DT_DEMO schema\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a virtual warehouse\n",
        "warehouse_sql = \"\"\"\n",
        "CREATE WAREHOUSE IF NOT EXISTS XSMALL_WH \n",
        "WAREHOUSE_TYPE = STANDARD\n",
        "WAREHOUSE_SIZE = XSMALL\n",
        "AUTO_SUSPEND = 300\n",
        "AUTO_RESUME = TRUE;\n",
        "\"\"\"\n",
        "\n",
        "execute_sql(warehouse_sql, \"Created XSMALL_WH warehouse\")\n",
        "\n",
        "# Use the warehouse\n",
        "execute_sql(\"USE WAREHOUSE XSMALL_WH;\", \"Using XSMALL_WH warehouse\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Create Sample Data with Python UDTFs\n",
        "\n",
        "Dynamic Tables work best with realistic data. Let's create sample datasets using Python User-Defined Table Functions (UDTFs). We'll create three main tables:\n",
        "1. **Customer Information** - Basic customer details\n",
        "2. **Product Stock Inventory** - Product catalog and stock levels\n",
        "3. **Sales Data** - Transaction records\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Customer Information Table\n",
        "\n",
        "First, let's create a Python UDTF to generate customer data:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Customer Info UDTF\n",
        "customer_udtf_sql = \"\"\"\n",
        "CREATE OR REPLACE FUNCTION gen_cust_info(num_records NUMBER)\n",
        "RETURNS TABLE (custid NUMBER(10), cname VARCHAR(100), spendlimit NUMBER(10,2))\n",
        "LANGUAGE PYTHON\n",
        "RUNTIME_VERSION=3.10\n",
        "HANDLER='CustTab'\n",
        "PACKAGES = ('Faker')\n",
        "AS $$\n",
        "from faker import Faker\n",
        "import random\n",
        "\n",
        "fake = Faker()\n",
        "\n",
        "class CustTab:\n",
        "    def process(self, num_records):\n",
        "        customer_id = 1000\n",
        "        for _ in range(num_records):\n",
        "            custid = customer_id + 1\n",
        "            cname = fake.name()\n",
        "            spendlimit = round(random.uniform(1000, 10000), 2)\n",
        "            customer_id += 1\n",
        "            yield (custid, cname, spendlimit)\n",
        "$$;\n",
        "\"\"\"\n",
        "\n",
        "execute_sql(customer_udtf_sql, \"Created gen_cust_info UDTF\")\n",
        "\n",
        "# Create the customer info table with 1000 records\n",
        "create_cust_table_sql = \"\"\"\n",
        "CREATE OR REPLACE TABLE cust_info AS \n",
        "SELECT * FROM TABLE(gen_cust_info(1000)) ORDER BY 1;\n",
        "\"\"\"\n",
        "\n",
        "execute_sql(create_cust_table_sql, \"Created cust_info table with 1000 customers\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's preview the customer data\n",
        "execute_sql(\"SELECT * FROM cust_info LIMIT 10;\", \"Preview of customer data\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Product Stock Inventory Table\n",
        "\n",
        "Now let's create a product inventory table with stock levels:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Product Stock Inventory UDTF\n",
        "product_udtf_sql = \"\"\"\n",
        "CREATE OR REPLACE FUNCTION gen_prod_stock_inv(num_records NUMBER)\n",
        "RETURNS TABLE (pid NUMBER(5), product_name VARCHAR(100), stock NUMBER(5))\n",
        "LANGUAGE PYTHON\n",
        "RUNTIME_VERSION=3.10\n",
        "HANDLER='ProdTab'\n",
        "PACKAGES = ('Faker')\n",
        "AS $$\n",
        "from faker import Faker\n",
        "import random\n",
        "\n",
        "fake = Faker()\n",
        "\n",
        "class ProdTab:\n",
        "    def process(self, num_records):\n",
        "        product_types = ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Headphones', \n",
        "                        'Tablet', 'Phone', 'Printer', 'Camera', 'Speaker',\n",
        "                        'Router', 'Cable', 'Charger', 'Webcam', 'Microphone']\n",
        "        \n",
        "        for product_id in range(1, num_records + 1):\n",
        "            product_base = random.choice(product_types)\n",
        "            product_name = f\"{fake.company()} {product_base} {fake.bothify('###')}\"\n",
        "            stock = random.randint(50, 500)\n",
        "            yield (product_id, product_name, stock)\n",
        "$$;\n",
        "\"\"\"\n",
        "\n",
        "execute_sql(product_udtf_sql, \"Created gen_prod_stock_inv UDTF\")\n",
        "\n",
        "# Create the product stock inventory table\n",
        "create_product_table_sql = \"\"\"\n",
        "CREATE OR REPLACE TABLE prod_stock_inv AS \n",
        "SELECT * FROM TABLE(gen_prod_stock_inv(100)) ORDER BY 1;\n",
        "\"\"\"\n",
        "\n",
        "execute_sql(create_product_table_sql, \"Created prod_stock_inv table with 100 products\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Sales Data Table\n",
        "\n",
        "Finally, let's create sales transaction data with JSON structure:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Sales Data UDTF\n",
        "sales_udtf_sql = \"\"\"\n",
        "CREATE OR REPLACE FUNCTION gen_sales_data(num_records NUMBER)\n",
        "RETURNS TABLE (custid NUMBER(10), purchase VARIANT, creationtime TIMESTAMP)\n",
        "LANGUAGE PYTHON\n",
        "RUNTIME_VERSION=3.10\n",
        "HANDLER='SalesTab'\n",
        "PACKAGES = ('Faker')\n",
        "AS $$\n",
        "from faker import Faker\n",
        "import random\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "fake = Faker()\n",
        "\n",
        "class SalesTab:\n",
        "    def process(self, num_records):\n",
        "        # Generate sales data over the past 30 days\n",
        "        end_date = datetime.now()\n",
        "        start_date = end_date - timedelta(days=30)\n",
        "        \n",
        "        for _ in range(num_records):\n",
        "            custid = random.randint(1001, 2000)  # Customer IDs from our range\n",
        "            prodid = random.randint(1, 100)     # Product IDs from our range\n",
        "            quantity = random.randint(1, 10)\n",
        "            unit_price = round(random.uniform(10, 500), 2)\n",
        "            purchase_amount = round(quantity * unit_price, 2)\n",
        "            \n",
        "            # Random date within the last 30 days\n",
        "            purchase_date = fake.date_between(start_date=start_date, end_date=end_date)\n",
        "            \n",
        "            # Create JSON purchase data\n",
        "            purchase_json = {\n",
        "                \"prodid\": prodid,\n",
        "                \"quantity\": quantity,\n",
        "                \"unit_price\": unit_price,\n",
        "                \"purchase_amount\": purchase_amount,\n",
        "                \"purchase_date\": purchase_date.strftime('%Y-%m-%d')\n",
        "            }\n",
        "            \n",
        "            # Random timestamp for creation\n",
        "            creationtime = fake.date_time_between(start_date=purchase_date, end_date=end_date)\n",
        "            \n",
        "            yield (custid, json.dumps(purchase_json), creationtime)\n",
        "$$;\n",
        "\"\"\"\n",
        "\n",
        "execute_sql(sales_udtf_sql, \"Created gen_sales_data UDTF\")\n",
        "\n",
        "# Create the sales data table\n",
        "create_sales_table_sql = \"\"\"\n",
        "CREATE OR REPLACE TABLE salesdata AS \n",
        "SELECT * FROM TABLE(gen_sales_data(5000)) ORDER BY creationtime;\n",
        "\"\"\"\n",
        "\n",
        "execute_sql(create_sales_table_sql, \"Created salesdata table with 5000 transactions\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preview the sales data structure\n",
        "execute_sql(\"SELECT * FROM salesdata LIMIT 5;\", \"Preview of sales data\")\n",
        "\n",
        "# Let's also check our product inventory\n",
        "execute_sql(\"SELECT * FROM prod_stock_inv LIMIT 10;\", \"Preview of product inventory\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Creating Dynamic Tables\n",
        "\n",
        "Dynamic Tables automatically maintain the results of a query as underlying data changes. Let's create several dynamic tables to build a data pipeline.\n",
        "\n",
        "### Key Concepts:\n",
        "- **LAG**: How much delay is acceptable between source data changes and updates to the dynamic table\n",
        "- **WAREHOUSE**: The compute warehouse used for refreshes\n",
        "- **Dependencies**: Dynamic tables can depend on other tables and dynamic tables\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Customer Sales Data History\n",
        "\n",
        "Our first dynamic table will combine customer and sales data, extracting JSON fields:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create customer sales data history dynamic table\n",
        "customer_sales_dt_sql = \"\"\"\n",
        "CREATE OR REPLACE DYNAMIC TABLE customer_sales_data_history\n",
        "LAG='DOWNSTREAM'\n",
        "WAREHOUSE=XSMALL_WH\n",
        "AS\n",
        "SELECT \n",
        "    s.custid AS customer_id,\n",
        "    c.cname AS customer_name,\n",
        "    s.purchase:\"prodid\"::NUMBER(5) AS product_id,\n",
        "    s.purchase:\"purchase_amount\"::NUMBER(10) AS saleprice,\n",
        "    s.purchase:\"quantity\"::NUMBER(5) AS quantity,\n",
        "    s.purchase:\"purchase_date\"::DATE AS salesdate,\n",
        "    s.creationtime\n",
        "FROM\n",
        "    cust_info c \n",
        "    INNER JOIN salesdata s ON c.custid = s.custid;\n",
        "\"\"\"\n",
        "\n",
        "execute_sql(customer_sales_dt_sql, \"Created customer_sales_data_history dynamic table\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preview the dynamic table\n",
        "execute_sql(\"SELECT * FROM customer_sales_data_history LIMIT 10;\", \"Preview of customer sales data history\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Sales Report Dynamic Table\n",
        "\n",
        "Let's create a sales report that enriches our sales data with product information:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sales report dynamic table\n",
        "sales_report_dt_sql = \"\"\"\n",
        "CREATE OR REPLACE DYNAMIC TABLE salesreport\n",
        "LAG='DOWNSTREAM'\n",
        "WAREHOUSE=XSMALL_WH\n",
        "AS\n",
        "SELECT \n",
        "    csdh.customer_id,\n",
        "    csdh.customer_name,\n",
        "    csdh.product_id,\n",
        "    psi.product_name,\n",
        "    psi.stock,\n",
        "    csdh.saleprice,\n",
        "    csdh.quantity,\n",
        "    csdh.salesdate,\n",
        "    csdh.creationtime\n",
        "FROM\n",
        "    customer_sales_data_history csdh\n",
        "    INNER JOIN prod_stock_inv psi ON csdh.product_id = psi.pid;\n",
        "\"\"\"\n",
        "\n",
        "execute_sql(sales_report_dt_sql, \"Created salesreport dynamic table\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Cumulative Purchase Dynamic Table\n",
        "\n",
        "This dynamic table will track cumulative purchases by customer:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create cumulative purchase dynamic table\n",
        "cumulative_purchase_dt_sql = \"\"\"\n",
        "CREATE OR REPLACE DYNAMIC TABLE cumulative_purchase\n",
        "LAG='DOWNSTREAM'\n",
        "WAREHOUSE=XSMALL_WH\n",
        "AS\n",
        "SELECT \n",
        "    customer_id,\n",
        "    customer_name,\n",
        "    SUM(saleprice) AS total_spent,\n",
        "    COUNT(*) AS total_purchases,\n",
        "    AVG(saleprice) AS avg_purchase_amount,\n",
        "    MIN(salesdate) AS first_purchase_date,\n",
        "    MAX(salesdate) AS latest_purchase_date,\n",
        "    MAX(creationtime) AS last_updated\n",
        "FROM\n",
        "    customer_sales_data_history\n",
        "GROUP BY \n",
        "    customer_id, customer_name;\n",
        "\"\"\"\n",
        "\n",
        "execute_sql(cumulative_purchase_dt_sql, \"Created cumulative_purchase dynamic table\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preview cumulative purchase data\n",
        "execute_sql(\"SELECT * FROM cumulative_purchase ORDER BY total_spent DESC LIMIT 10;\", \"Top 10 customers by total spending\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Data Validation and Alerts\n",
        "\n",
        "One powerful use case for Dynamic Tables is monitoring data quality and creating alerts. Let's create a dynamic table that monitors product inventory levels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create product inventory alert dynamic table\n",
        "prod_inv_alert_dt_sql = \"\"\"\n",
        "CREATE OR REPLACE DYNAMIC TABLE PROD_INV_ALERT\n",
        "LAG = '1 MINUTE'\n",
        "WAREHOUSE=XSMALL_WH\n",
        "AS\n",
        "SELECT \n",
        "    S.PRODUCT_ID, \n",
        "    S.PRODUCT_NAME,\n",
        "    S.CREATIONTIME AS LATEST_SALES_DATE,\n",
        "    S.STOCK AS BEGINNING_STOCK,\n",
        "    SUM(S.QUANTITY) OVER (PARTITION BY S.PRODUCT_ID ORDER BY S.CREATIONTIME) AS TOTALUNITSOLD, \n",
        "    (S.STOCK - SUM(S.QUANTITY) OVER (PARTITION BY S.PRODUCT_ID ORDER BY S.CREATIONTIME)) AS UNITSLEFT,\n",
        "    ROUND(((S.STOCK - SUM(S.QUANTITY) OVER (PARTITION BY S.PRODUCT_ID ORDER BY S.CREATIONTIME)) / S.STOCK) * 100, 2) AS PERCENT_UNITLEFT,\n",
        "    CURRENT_TIMESTAMP() AS ROWCREATIONTIME\n",
        "FROM SALESREPORT S \n",
        "QUALIFY ROW_NUMBER() OVER (PARTITION BY PRODUCT_ID ORDER BY CREATIONTIME DESC) = 1;\n",
        "\"\"\"\n",
        "\n",
        "execute_sql(prod_inv_alert_dt_sql, \"Created PROD_INV_ALERT dynamic table\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for low inventory products (less than 10% remaining)\n",
        "execute_sql(\"SELECT * FROM PROD_INV_ALERT WHERE PERCENT_UNITLEFT < 10 ORDER BY PERCENT_UNITLEFT;\", \"Products with low inventory (< 10%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Monitoring and Managing Dynamic Tables\n",
        "\n",
        "Snowflake provides several ways to monitor your dynamic tables and understand their refresh patterns.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 Check Dynamic Table Refresh History\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check dynamic table refresh history\n",
        "refresh_history_sql = \"\"\"\n",
        "SELECT * \n",
        "FROM TABLE(INFORMATION_SCHEMA.DYNAMIC_TABLE_REFRESH_HISTORY())\n",
        "WHERE NAME IN ('SALESREPORT', 'CUSTOMER_SALES_DATA_HISTORY', 'PROD_INV_ALERT', 'CUMULATIVE_PURCHASE')\n",
        "ORDER BY DATA_TIMESTAMP DESC, REFRESH_END_TIME DESC\n",
        "LIMIT 10;\n",
        "\"\"\"\n",
        "\n",
        "execute_sql(refresh_history_sql, \"Dynamic table refresh history\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 View Dynamic Table Properties\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View dynamic table properties\n",
        "dt_properties_sql = \"\"\"\n",
        "SHOW DYNAMIC TABLES IN SCHEMA DEMO.DT_DEMO;\n",
        "\"\"\"\n",
        "\n",
        "execute_sql(dt_properties_sql, \"Dynamic tables in our schema\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.3 Force Refresh a Dynamic Table\n",
        "\n",
        "You can manually refresh a dynamic table if needed:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Force refresh a dynamic table (uncomment to run)\n",
        "# execute_sql(\"ALTER DYNAMIC TABLE PROD_INV_ALERT REFRESH;\", \"Manually refreshed PROD_INV_ALERT\")\n",
        "\n",
        "print(\"💡 Tip: You can manually refresh any dynamic table using ALTER DYNAMIC TABLE <name> REFRESH;\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Exploring Your Data Pipeline\n",
        "\n",
        "Now that we have our dynamic tables set up, let's run some analytical queries to see the power of our automated data pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sales summary by customer\n",
        "execute_sql(\"\"\"\n",
        "SELECT \n",
        "    customer_name,\n",
        "    total_purchases,\n",
        "    total_spent,\n",
        "    avg_purchase_amount,\n",
        "    first_purchase_date,\n",
        "    latest_purchase_date\n",
        "FROM cumulative_purchase \n",
        "ORDER BY total_spent DESC \n",
        "LIMIT 15;\n",
        "\"\"\", \"Top 15 customers by total spending\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Product performance analysis\n",
        "execute_sql(\"\"\"\n",
        "SELECT \n",
        "    product_name,\n",
        "    COUNT(*) as total_sales,\n",
        "    SUM(quantity) as units_sold,\n",
        "    SUM(saleprice) as total_revenue,\n",
        "    AVG(saleprice) as avg_sale_price\n",
        "FROM salesreport \n",
        "GROUP BY product_name \n",
        "ORDER BY total_revenue DESC \n",
        "LIMIT 10;\n",
        "\"\"\", \"Top 10 products by revenue\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Daily sales trends\n",
        "execute_sql(\"\"\"\n",
        "SELECT \n",
        "    salesdate,\n",
        "    COUNT(*) as transactions,\n",
        "    SUM(saleprice) as daily_revenue,\n",
        "    SUM(quantity) as units_sold\n",
        "FROM customer_sales_data_history \n",
        "GROUP BY salesdate \n",
        "ORDER BY salesdate DESC \n",
        "LIMIT 15;\n",
        "\"\"\", \"Daily sales trends (last 15 days)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Dynamic Table Pipeline Visualization\n",
        "\n",
        "In Snowsight (Snowflake's web interface), you can visualize your dynamic table pipeline:\n",
        "\n",
        "1. Navigate to **Data > Databases > DEMO > DT_DEMO > Dynamic Tables**\n",
        "2. Click on any dynamic table to view its directed acyclic graph (DAG)\n",
        "3. This shows dependencies between tables and refresh status\n",
        "\n",
        "The pipeline we've created looks like this:\n",
        "\n",
        "```\n",
        "[Base Tables]           [Dynamic Tables]           [Alert Tables]\n",
        "cust_info      ─────┐\n",
        "                     ├──> customer_sales_data_history ──┐\n",
        "salesdata      ─────┘                                  │\n",
        "                                                        ├──> salesreport ──> PROD_INV_ALERT\n",
        "prod_stock_inv ─────────────────────────────────────┘\n",
        "                                                        \n",
        "customer_sales_data_history ────> cumulative_purchase\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Best Practices and Optimization\n",
        "\n",
        "### LAG Settings\n",
        "- **DOWNSTREAM**: Refreshes when downstream dependencies need it\n",
        "- **Specific time**: e.g., '10 minutes', '1 hour' for regular refresh intervals\n",
        "\n",
        "### Performance Tips\n",
        "1. Use appropriate LAG settings based on your business requirements\n",
        "2. Right-size your warehouse for dynamic table refreshes\n",
        "3. Consider the dependency chain when designing your pipeline\n",
        "4. Monitor refresh history to optimize performance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Cleanup (Optional)\n",
        "\n",
        "If you want to clean up the resources created in this demo, uncomment and run the following cell:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cleanup commands (uncomment to execute)\n",
        "cleanup_commands = [\n",
        "    # \"DROP DYNAMIC TABLE IF EXISTS PROD_INV_ALERT;\",\n",
        "    # \"DROP DYNAMIC TABLE IF EXISTS cumulative_purchase;\",\n",
        "    # \"DROP DYNAMIC TABLE IF EXISTS salesreport;\", \n",
        "    # \"DROP DYNAMIC TABLE IF EXISTS customer_sales_data_history;\",\n",
        "    # \"DROP TABLE IF EXISTS salesdata;\",\n",
        "    # \"DROP TABLE IF EXISTS prod_stock_inv;\",\n",
        "    # \"DROP TABLE IF EXISTS cust_info;\",\n",
        "    # \"DROP FUNCTION IF EXISTS gen_sales_data(NUMBER);\",\n",
        "    # \"DROP FUNCTION IF EXISTS gen_prod_stock_inv(NUMBER);\",\n",
        "    # \"DROP FUNCTION IF EXISTS gen_cust_info(NUMBER);\",\n",
        "    # \"DROP WAREHOUSE IF EXISTS XSMALL_WH;\",\n",
        "    # \"DROP SCHEMA IF EXISTS DEMO.DT_DEMO;\",\n",
        "    # \"DROP DATABASE IF EXISTS DEMO;\"\n",
        "]\n",
        "\n",
        "# for cmd in cleanup_commands:\n",
        "#     execute_sql(cmd, f\"Executed: {cmd}\")\n",
        "\n",
        "print(\"💡 Uncomment and run the above commands if you want to clean up all demo resources.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "Congratulations! 🎉 You've successfully completed the Snowflake Dynamic Tables quickstart. In this tutorial, you've learned how to:\n",
        "\n",
        "✅ **Set up your Snowflake environment** with databases, schemas, and warehouses  \n",
        "✅ **Generate realistic sample data** using Python UDTFs with the Faker library  \n",
        "✅ **Create dynamic tables** that automatically maintain transformed data  \n",
        "✅ **Implement monitoring and alerts** for data quality and inventory management  \n",
        "✅ **Monitor and manage** your dynamic table pipeline  \n",
        "✅ **Run analytical queries** on your automated data pipeline  \n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **Dynamic Tables simplify data pipeline management** by automatically refreshing when source data changes\n",
        "2. **LAG settings** control refresh frequency and can be tuned based on business requirements  \n",
        "3. **Dependency chains** allow you to build complex data transformations with automatic propagation\n",
        "4. **Monitoring capabilities** help you track performance and ensure data quality\n",
        "5. **Integration with Snowsight** provides visual pipeline management\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Explore more complex transformations using window functions and advanced SQL\n",
        "- Experiment with different LAG settings to balance freshness and cost\n",
        "- Integrate dynamic tables with Snowflake streams and tasks for even more automation\n",
        "- Set up alerts and notifications for critical business metrics\n",
        "\n",
        "### Resources\n",
        "\n",
        "- [Snowflake Dynamic Tables Documentation](https://docs.snowflake.com/en/user-guide/dynamic-tables-about)\n",
        "- [Dynamic Tables Best Practices](https://docs.snowflake.com/en/user-guide/dynamic-tables-best-practices)\n",
        "- [Snowflake Python Connector Documentation](https://docs.snowflake.com/en/developer-guide/python-connector/python-connector)\n",
        "\n",
        "---\n",
        "\n",
        "*Happy coding with Snowflake Dynamic Tables!* ❄️\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
